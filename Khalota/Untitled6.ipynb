{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled6.ipynb","provenance":[],"authorship_tag":"ABX9TyPV+/zctJDCXK0RVFz18OJb"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"LoWBY2uAf7o2"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CIfzKppwioh","executionInfo":{"status":"ok","timestamp":1604593197697,"user_tz":-60,"elapsed":14148,"user":{"displayName":"nazim bendib","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVBYzWIbBcg68T0fUA6Zs2zmU2ibf0mW1WTq70=s64","userId":"15864549587565971050"}},"outputId":"3398a0b0-2d8c-463a-8ed5-0076bfb17f95","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install sentence_transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting sentence_transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/fd/0190080aa0af78d7cd5874e4e8e85f0bed9967dd387cf05d760832b95da9/sentence-transformers-0.3.8.tar.gz (66kB)\n","\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n","\u001b[?25hCollecting transformers<3.4.0,>=3.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 7.5MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (4.41.1)\n","Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (1.7.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (1.18.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence_transformers) (3.2.5)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence_transformers) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence_transformers) (2.23.0)\n","Collecting tokenizers==0.8.1.rc2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 23.9MB/s \n","\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence_transformers) (0.0.43)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence_transformers) (0.1.94)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence_transformers) (20.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence_transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence_transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence_transformers) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence_transformers) (0.16.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence_transformers) (0.17.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence_transformers) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence_transformers) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence_transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence_transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence_transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.4.0,>=3.1.0->sentence_transformers) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence_transformers) (2.4.7)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-cp36-none-any.whl size=101996 sha256=7f27003561427ea348e19be643d7eae620ee594a501703908c7ebd6489972eff\n","  Stored in directory: /root/.cache/pip/wheels/27/ec/b3/d12cc8e4daf77846db6543033d3a5642f204c0320b15945647\n","Successfully built sentence-transformers\n","Installing collected packages: tokenizers, transformers, sentence-transformers\n","  Found existing installation: tokenizers 0.9.2\n","    Uninstalling tokenizers-0.9.2:\n","      Successfully uninstalled tokenizers-0.9.2\n","  Found existing installation: transformers 3.4.0\n","    Uninstalling transformers-3.4.0:\n","      Successfully uninstalled transformers-3.4.0\n","Successfully installed sentence-transformers-0.3.8 tokenizers-0.8.1rc2 transformers-3.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bmXhEcFdgMuI"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","import transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"By-oiKGlgjvK"},"source":["text =  \"\"\" Supervised learning is the machine learning task of  learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2] In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a 'reasonable' way (see inductive bias).\n","      \"\"\"\n","text = text.lower()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dkNh32PY7rY5"},"source":["We first find the words/phrases that seem to be keywords/keyphrases"]},{"cell_type":"code","metadata":{"id":"DE6eh8-s6U5_","executionInfo":{"status":"ok","timestamp":1604593248984,"user_tz":-60,"elapsed":4000,"user":{"displayName":"nazim bendib","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVBYzWIbBcg68T0fUA6Zs2zmU2ibf0mW1WTq70=s64","userId":"15864549587565971050"}},"outputId":"5cb376e3-d5ec-4831-b6e7-b02b7b1e4cd5","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install pytextrank"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pytextrank\n","  Downloading https://files.pythonhosted.org/packages/38/97/3cd3ec7396b6cb5984d860b18a8741f59d4652c8bbfc7b7431e9831e5d53/pytextrank-2.0.3-py3-none-any.whl\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from pytextrank) (2.2.4)\n","Requirement already satisfied: coverage in /usr/local/lib/python3.6/dist-packages (from pytextrank) (3.7.1)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from pytextrank) (0.10.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from pytextrank) (2.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (2.23.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (1.0.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (3.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (0.8.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (1.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (50.3.2)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (0.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (1.18.5)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (7.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (2.0.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (1.0.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (4.41.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->pytextrank) (1.1.3)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->pytextrank) (4.4.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->pytextrank) (2020.6.20)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->pytextrank) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->pytextrank) (3.4.0)\n","Installing collected packages: pytextrank\n","Successfully installed pytextrank-2.0.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AbGTT_446bZ5"},"source":["import spacy\n","import pytextrank\n","\n","# example text\n","#text = \"machine learning is the most important task in artificial inteligence so in order to train your model you should have a larg, very large dataset\"\n","\n","\n","# load a spaCy model, depending on language, scale, etc.\n","\n","def get_phrases(text,rate=0.05, printed=False):\n","    nlp = spacy.load(\"en_core_web_sm\")\n","\n","    # add PyTextRank to the spaCy pipeline\n","    tr = pytextrank.TextRank()\n","    nlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n","\n","    doc = nlp(text)\n","\n","    # examine the top-ranked phrases in the document\n","    if printed:\n","        for p in doc._.phrases:\n","            print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n","            print(p.chunks)\n","\n","    return [p.text for p in doc._.phrases if p.rank >= rate ]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3j6ZCgdHokYh"},"source":["\n","def XXX(text, candidates, rate=0.05):\n","    from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","    from sentence_transformers import SentenceTransformer\n","    from transformers import BertTokenizer\n","    \n","    #load the SBERT pre trained model\n","    sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n","\n","    #get the text embedding\n","    text_embedding = sbert_model.encode([text])\n","\n","    #get the candidates embeddings\n","    candidates_embeddings = sbert_model.encode(candidates)\n","\n","    #calcule the distances between the text and the candidates\n","    distances = cosine_similarity(text_embedding, candidates_embeddings)\n","\n","    #sort and take the n_top candidates with low distances \n","    \n","    keywords = [candidates[index] for index in distances.argsort()[0] if distances[0][index] >= rate]\n","    #keywords = [index for index in distances.argsort()[0][-n_top:]]\n","\n","    return keywords\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2gDFmvjpvL9I"},"source":["text = \"ESI is the is the international school of IT, it's the biggest school in algeria so far, you enter it with 18\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOdFZVsPnnQx","executionInfo":{"status":"ok","timestamp":1604593250535,"user_tz":-60,"elapsed":5518,"user":{"displayName":"nazim bendib","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVBYzWIbBcg68T0fUA6Zs2zmU2ibf0mW1WTq70=s64","userId":"15864549587565971050"}},"outputId":"1abfec3a-d62b-4abd-90fe-55aed0f7b6fb","colab":{"base_uri":"https://localhost:8080/"}},"source":["candidates = get_phrases(text, rate=0.09)\n","print(candidates)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['algeria', 'the international school', 'the biggest school']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iMzKlnaWsYbd","executionInfo":{"status":"ok","timestamp":1604593276368,"user_tz":-60,"elapsed":31344,"user":{"displayName":"nazim bendib","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVBYzWIbBcg68T0fUA6Zs2zmU2ibf0mW1WTq70=s64","userId":"15864549587565971050"}},"outputId":"afa46198-8236-4492-a616-28c1965f1cb1","colab":{"base_uri":"https://localhost:8080/"}},"source":["keywords = XXX(text, candidates, rate=0.1)\n","print(keywords)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 405M/405M [00:17<00:00, 23.2MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["['algeria', 'the international school', 'the biggest school']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y-7HLef6JSBb","executionInfo":{"status":"ok","timestamp":1604700989717,"user_tz":-60,"elapsed":6873,"user":{"displayName":"nazim bendib","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVBYzWIbBcg68T0fUA6Zs2zmU2ibf0mW1WTq70=s64","userId":"15864549587565971050"}},"outputId":"324e21b8-259e-4b61-8b71-52a012f25190","colab":{"base_uri":"https://localhost:8080/"}},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('treebank')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package treebank to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/treebank.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"flHlJEmKJVKm","executionInfo":{"status":"ok","timestamp":1604700998228,"user_tz":-60,"elapsed":1126,"user":{"displayName":"nazim bendib","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgVBYzWIbBcg68T0fUA6Zs2zmU2ibf0mW1WTq70=s64","userId":"15864549587565971050"}},"outputId":"83bb21bb-c87e-4b37-feee-898c2831dcfe","colab":{"base_uri":"https://localhost:8080/"}},"source":["text = \"\"\"The science of physiology, which is concerned with the normal functions of the animal body, is the basis of medicine and surgery, which are concerned with the abnormal functions of the human body.\"\"\"\n","tokens = nltk.word_tokenize(text)\n","tagged = nltk.pos_tag(tokens)\n","print(tagged)\n","entities = nltk.chunk.ne_chunk(tagged)\n","print(entities)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[('The', 'DT'), ('science', 'NN'), ('of', 'IN'), ('physiology', 'NN'), (',', ','), ('which', 'WDT'), ('is', 'VBZ'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('normal', 'JJ'), ('functions', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('animal', 'NN'), ('body', 'NN'), (',', ','), ('is', 'VBZ'), ('the', 'DT'), ('basis', 'NN'), ('of', 'IN'), ('medicine', 'NN'), ('and', 'CC'), ('surgery', 'NN'), (',', ','), ('which', 'WDT'), ('are', 'VBP'), ('concerned', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('abnormal', 'JJ'), ('functions', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('human', 'JJ'), ('body', 'NN'), ('.', '.')]\n","(S\n","  The/DT\n","  science/NN\n","  of/IN\n","  physiology/NN\n","  ,/,\n","  which/WDT\n","  is/VBZ\n","  concerned/VBN\n","  with/IN\n","  the/DT\n","  normal/JJ\n","  functions/NNS\n","  of/IN\n","  the/DT\n","  animal/NN\n","  body/NN\n","  ,/,\n","  is/VBZ\n","  the/DT\n","  basis/NN\n","  of/IN\n","  medicine/NN\n","  and/CC\n","  surgery/NN\n","  ,/,\n","  which/WDT\n","  are/VBP\n","  concerned/VBN\n","  with/IN\n","  the/DT\n","  abnormal/JJ\n","  functions/NNS\n","  of/IN\n","  the/DT\n","  human/JJ\n","  body/NN\n","  ./.)\n"],"name":"stdout"}]}]}